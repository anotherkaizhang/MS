{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_file, preprocessing, get_vocab, load_embeddings, create_gows, accuracy, generate_batches, AverageMeter\n",
    "from models import MPAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    path_to_dataset='../../../MS_shayan/data_for_mpad.txt'\n",
    "    path_to_embeddings = '../../../MS_shayan/Github_MedicalWordEmbeddings/BioWordVec_PubMed_MIMICIII_d200.vec.bin'\n",
    "    no_cuda=False\n",
    "    epochs=200\n",
    "    lr=0.001\n",
    "    hidden=64\n",
    "    penultimate=64\n",
    "    message_passing_layers=2\n",
    "    window_size=10\n",
    "    directed=True\n",
    "    use_master_node=True\n",
    "    normalize=True\n",
    "    dropout=0.5\n",
    "    batch_size=128\n",
    "    patience=20\n",
    "\n",
    "args = Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "docs, class_labels = load_file(args.path_to_dataset)\n",
    "print(len(docs), len(class_labels))\n",
    "print(docs[:2], class_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = preprocessing(docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "class_labels = enc.fit_transform(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclass = np.unique(class_labels).size\n",
    "y = list()\n",
    "for i in range(len(class_labels)):\n",
    "    t = np.zeros(1)\n",
    "    t[0] = class_labels[i]\n",
    "    y.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, confusion_matrix , auc, precision_recall_curve, average_precision_score\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def print_metrics(y_true, y_pred):\n",
    "    false_positive_rate, recall, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    auprc = average_precision_score(y_true, y_pred)\n",
    "    print('AUC: ',roc_auc, \"AUPRC: \", auprc)\n",
    "\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "\n",
    "    # calculate the g-mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred > thresholds[ix]).ravel()\n",
    "    print('TN: ', tn, \", FP: \",fp, \", FN:\", fn, \", TP:\", tp)\n",
    "    print(\"==> Sensitivity (Recall, TPR): %.3f\"%(tp/(tp+fn)))\n",
    "    print(\"==> Specifity: %.3f\"%(tn/(tn+fp)))\n",
    "    print(\"==> Positive Predictive Value (PPV) (Precision): %.3f\"%(tp / (tp + fp)))\n",
    "    print(\"==> Negative Predictive Value (NPV): %.3f\"%(tn / (tn + fn)))\n",
    "    print(\"==> Accuracy: %.3f\"%((tp+tn)/(tn+ fp+ fn+tp)))\n",
    "    print(\"==> F1 score: %.3f\"%((2*tp)/(2*tp + fp + fn)))\n",
    "\n",
    "    ns_probs = [0 for _ in range(len(y_true))]\n",
    "    \n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_true, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_true, y_pred)\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill, AUC = %0.2f' % 0.5)\n",
    "    plt.plot(lr_fpr, lr_tpr, marker='.', label = 'Our model: AUC = %0.2f' % roc_auc)\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    # summarize scores\n",
    "#     print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "    # plot the precision-recall curves\n",
    "    y_true = np.array(y_true)\n",
    "    no_skill = len(y_true[y_true==1]) / len(y_true)\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No skill, AUPRC = %0.2f' % no_skill)\n",
    "    plt.plot(lr_recall, lr_precision, marker='.',label = 'Our model: AUPRC = %0.2f' % auprc)\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend(loc = 'upper right')\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(docs)\n",
    "embeddings = load_embeddings(args.path_to_embeddings, vocab)\n",
    "adj, features, _ = create_gows(docs, vocab, args.window_size, args.directed, args.normalize, args.use_master_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab), embeddings.shape, len(adj),  len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'check tokens :', vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(docs)):\n",
    "#     print(len(docs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# already saved, no need retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=7, shuffle=True, random_state=0)\n",
    "it = 0\n",
    "accs = list()\n",
    "\n",
    "## for averaging AUC\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(y):\n",
    "    it += 1\n",
    "\n",
    "    idx = np.random.permutation(train_index)\n",
    "    train_index = idx[:int(idx.size*0.9)].tolist()\n",
    "    val_index = idx[int(idx.size*0.9):].tolist()\n",
    "\n",
    "    n_train = len(train_index)\n",
    "    n_val = len(val_index)\n",
    "    n_test = len(test_index)\n",
    "\n",
    "    adj_train = [adj[i] for i in train_index]\n",
    "    features_train = [features[i] for i in train_index]\n",
    "    y_train = [y[i] for i in train_index]\n",
    "\n",
    "    adj_val = [adj[i] for i in val_index]\n",
    "    features_val = [features[i] for i in val_index]\n",
    "    y_val = [y[i] for i in val_index]\n",
    "\n",
    "    adj_test = [adj[i] for i in test_index]\n",
    "    features_test = [features[i] for i in test_index]\n",
    "    y_test = [y[i] for i in test_index]\n",
    "\n",
    "    adj_train, features_train, batch_n_graphs_train, y_train = generate_batches(adj_train, features_train, y_train, args.batch_size, args.use_master_node)\n",
    "    adj_val, features_val, batch_n_graphs_val, y_val = generate_batches(adj_val, features_val, y_val, args.batch_size, args.use_master_node)\n",
    "    adj_test, features_test, batch_n_graphs_test, y_test = generate_batches(adj_test, features_test, y_test, args.batch_size, args.use_master_node)\n",
    "\n",
    "    n_train_batches = ceil(n_train/args.batch_size)\n",
    "    n_val_batches = ceil(n_val/args.batch_size)\n",
    "    n_test_batches = ceil(n_test/args.batch_size)\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = MPAD(embeddings.shape[1], args.message_passing_layers, args.hidden, args.penultimate, nclass, args.dropout, embeddings, args.use_master_node)\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        adj_train = [x.cuda() for x in adj_train]\n",
    "        features_train = [x.cuda() for x in features_train]\n",
    "        batch_n_graphs_train = [x.cuda() for x in batch_n_graphs_train]\n",
    "        y_train = [x.cuda() for x in y_train]\n",
    "        adj_val = [x.cuda() for x in adj_val]\n",
    "        features_val = [x.cuda() for x in features_val]\n",
    "        batch_n_graphs_val = [x.cuda() for x in batch_n_graphs_val]\n",
    "        y_val = [x.cuda() for x in y_val]\n",
    "        adj_test = [x.cuda() for x in adj_test]\n",
    "        features_test = [x.cuda() for x in features_test]\n",
    "        batch_n_graphs_test = [x.cuda() for x in batch_n_graphs_test]\n",
    "        y_test = [x.cuda() for x in y_test]\n",
    "\n",
    "    def train(epoch, adj, features, batch_n_graphs, y):\n",
    "        optimizer.zero_grad()\n",
    "#         print('features.shape, ', features.shape)\n",
    "        output = model(features, adj, batch_n_graphs)\n",
    "#         print(output.shape)\n",
    "#         input('efefe')\n",
    "        loss_train = F.cross_entropy(output, y)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        return output, loss_train\n",
    "\n",
    "    def test(adj, features, batch_n_graphs, y):\n",
    "        output = model(features, adj, batch_n_graphs)\n",
    "        loss_test = F.cross_entropy(output, y)\n",
    "        return output, loss_test\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        scheduler.step()\n",
    "        \n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = AverageMeter()\n",
    "        train_acc = AverageMeter()\n",
    "\n",
    "        # Train for one epoch\n",
    "        for i in range(n_train_batches):\n",
    "            output, loss = train(epoch, adj_train[i], features_train[i], batch_n_graphs_train[i], y_train[i])\n",
    "            train_loss.update(loss.item(), output.size(0))\n",
    "            train_acc.update(accuracy(output.data, y_train[i].data), output.size(0))\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = AverageMeter()\n",
    "        val_acc = AverageMeter()\n",
    "\n",
    "        for i in range(n_val_batches):\n",
    "            output, loss = test(adj_val[i], features_val[i], batch_n_graphs_val[i], y_val[i])\n",
    "            val_loss.update(loss.item(), output.size(0))\n",
    "            val_acc.update(accuracy(output.data, y_val[i].data), output.size(0))\n",
    "    \n",
    "        # Print results\n",
    "        print(\"Cross-val iter:\", '%02d' % it, \"epoch:\", '%03d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(train_loss.avg),\n",
    "            \"train_acc=\", \"{:.5f}\".format(train_acc.avg), \"val_loss=\", \"{:.5f}\".format(val_loss.avg),\n",
    "            \"val_acc=\", \"{:.5f}\".format(val_acc.avg), \"time=\", \"{:.5f}\".format(time.time() - start))\n",
    "        \n",
    "        # Remember best accuracy and save checkpoint\n",
    "        is_best = val_acc.avg >= best_acc\n",
    "        best_acc = max(val_acc.avg, best_acc)\n",
    "        if is_best:\n",
    "            early_stopping_counter = 0\n",
    "            '''torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, 'model_best_'+str(it)+'.pth.tar')'''\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(\"EarlyStopping: %i / %i\" % (early_stopping_counter, args.patience))\n",
    "            if early_stopping_counter == args.patience:\n",
    "                print(\"EarlyStopping: Stop training\")\n",
    "                break\n",
    "\n",
    "    print(\"Optimization finished!\")\n",
    "\n",
    "    # Testing\n",
    "    test_loss = AverageMeter()\n",
    "    test_acc = AverageMeter()\n",
    "    print(\"Loading checkpoint!\")\n",
    "    checkpoint = torch.load('model_best_'+str(it)+'.pth.tar')\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    ## for averaging AUC\n",
    "    test_out_all = []\n",
    "    test_true_all = []\n",
    "    \n",
    "    for i in range(n_test_batches):\n",
    "        output, loss = test(adj_test[i], features_test[i], batch_n_graphs_test[i], y_test[i])\n",
    "        test_loss.update(loss.item(), output.size(0))\n",
    "        test_acc.update(accuracy(output.data, y_test[i].data), output.size(0))\n",
    "        # *****\n",
    "        test_out_all.extend(output.data.cpu()[:,1].tolist())\n",
    "        test_true_all.extend(y_test[i].data.cpu().tolist())\n",
    "        \n",
    "    accs.append(test_acc.avg.cpu().numpy())\n",
    "\n",
    "    # Print results\n",
    "    print(\"test_loss=\", \"{:.5f}\".format(test_loss.avg), \"test_acc=\", \"{:.5f}\".format(test_acc.avg))\n",
    "    print_metrics(test_true_all, test_out_all)\n",
    "    \n",
    "    viz_fpr, viz_tpr, thresholds = metrics.roc_curve(test_true_all, test_out_all)\n",
    "    interp_tpr = np.interp(mean_fpr, viz_fpr, viz_tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    false_positive_rate, recall, thresholds = roc_curve(test_true_all, test_out_all)\n",
    "    viz_roc_auc = auc(false_positive_rate, recall)\n",
    "    aucs.append(viz_roc_auc)\n",
    "    \n",
    "    \n",
    "    print()\n",
    "print(\"avg_test_acc=\", \"{:.5f}\".format(np.mean(accs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    title=\"Receiver operating characteristic example\",\n",
    ")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"avg_test_acc=\", \"{:.5f}\".format(np.mean(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save mpad embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "del MPAD\n",
    "from models import MPAD  \n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=False)\n",
    "it = 0\n",
    "accs = list()\n",
    "\n",
    "## for averaging AUC\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Run ONly once to get all embeddgins \n",
    "run_only_once = 1\n",
    "# Set batch to all data\n",
    "args.batch_size = 299\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(y):\n",
    "#     it += 1\n",
    "    all_index = np.array(range(0,299))\n",
    "    \n",
    "#     idx = np.random.permutation(train_index)\n",
    "#     train_index = idx[:int(idx.size*0.9)].tolist()\n",
    "#     val_index = idx[int(idx.size*0.9):].tolist()\n",
    "\n",
    "#     n_train = len(train_index)\n",
    "#     n_val = len(val_index)\n",
    "    n_test = len(all_index)\n",
    "\n",
    "#     adj_train = [adj[i] for i in train_index]\n",
    "#     features_train = [features[i] for i in train_index]\n",
    "#     y_train = [y[i] for i in train_index]\n",
    "\n",
    "#     adj_val = [adj[i] for i in val_index]\n",
    "#     features_val = [features[i] for i in val_index]\n",
    "#     y_val = [y[i] for i in val_index]\n",
    "\n",
    "    adj_test = [adj[i] for i in all_index]\n",
    "    features_test = [features[i] for i in all_index]\n",
    "    y_test = [y[i] for i in all_index]\n",
    "\n",
    "#     adj_train, features_train, batch_n_graphs_train, y_train = generate_batches(adj_train, features_train, y_train, args.batch_size, args.use_master_node)\n",
    "#     adj_val, features_val, batch_n_graphs_val, y_val = generate_batches(adj_val, features_val, y_val, args.batch_size, args.use_master_node)\n",
    "    adj_test, features_test, batch_n_graphs_test, y_test = generate_batches(adj_test, features_test, y_test, args.batch_size, args.use_master_node)\n",
    "\n",
    "#     n_train_batches = ceil(n_train/args.batch_size)\n",
    "#     n_val_batches = ceil(n_val/args.batch_size)\n",
    "    n_test_batches = ceil(n_test/args.batch_size)\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = MPAD(embeddings.shape[1], args.message_passing_layers, args.hidden, args.penultimate, nclass, args.dropout, embeddings, args.use_master_node)\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "#         adj_train = [x.cuda() for x in adj_train]\n",
    "#         features_train = [x.cuda() for x in features_train]\n",
    "#         batch_n_graphs_train = [x.cuda() for x in batch_n_graphs_train]\n",
    "#         y_train = [x.cuda() for x in y_train]\n",
    "#         adj_val = [x.cuda() for x in adj_val]\n",
    "#         features_val = [x.cuda() for x in features_val]\n",
    "#         batch_n_graphs_val = [x.cuda() for x in batch_n_graphs_val]\n",
    "#         y_val = [x.cuda() for x in y_val]\n",
    "        adj_test = [x.cuda() for x in adj_test]\n",
    "        features_test = [x.cuda() for x in features_test]\n",
    "        batch_n_graphs_test = [x.cuda() for x in batch_n_graphs_test]\n",
    "        y_test = [x.cuda() for x in y_test]\n",
    "\n",
    "#     def train(epoch, adj, features, batch_n_graphs, y):\n",
    "#         optimizer.zero_grad()\n",
    "#         print('features.shape, ', features.shape)\n",
    "#         output = model(features, adj, batch_n_graphs)\n",
    "#         print(output.shape)\n",
    "#         input('efefe')\n",
    "#         loss_train = F.cross_entropy(output, y)\n",
    "#         loss_train.backward()\n",
    "#         optimizer.step()\n",
    "#         return output, loss_train\n",
    "\n",
    "    def test(adj, features, batch_n_graphs, y):\n",
    "        output = model(features, adj, batch_n_graphs)\n",
    "        loss_test = F.cross_entropy(output, y)\n",
    "        return output, loss_test\n",
    "\n",
    "#     best_acc = 0\n",
    "\n",
    "#     for epoch in range(args.epochs):\n",
    "#         scheduler.step()\n",
    "        \n",
    "#         start = time.time()\n",
    "#         model.train()\n",
    "#         train_loss = AverageMeter()\n",
    "#         train_acc = AverageMeter()\n",
    "\n",
    "#         # Train for one epoch\n",
    "#         for i in range(n_train_batches):\n",
    "#             output, loss = train(epoch, adj_train[i], features_train[i], batch_n_graphs_train[i], y_train[i])\n",
    "#             train_loss.update(loss.item(), output.size(0))\n",
    "#             train_acc.update(accuracy(output.data, y_train[i].data), output.size(0))\n",
    "\n",
    "#         # Evaluate on validation set\n",
    "#         model.eval()\n",
    "#         val_loss = AverageMeter()\n",
    "#         val_acc = AverageMeter()\n",
    "\n",
    "#         for i in range(n_val_batches):\n",
    "#             output, loss = test(adj_val[i], features_val[i], batch_n_graphs_val[i], y_val[i])\n",
    "#             val_loss.update(loss.item(), output.size(0))\n",
    "#             val_acc.update(accuracy(output.data, y_val[i].data), output.size(0))\n",
    "    \n",
    "#         # Print results\n",
    "#         print(\"Cross-val iter:\", '%02d' % it, \"epoch:\", '%03d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(train_loss.avg),\n",
    "#             \"train_acc=\", \"{:.5f}\".format(train_acc.avg), \"val_loss=\", \"{:.5f}\".format(val_loss.avg),\n",
    "#             \"val_acc=\", \"{:.5f}\".format(val_acc.avg), \"time=\", \"{:.5f}\".format(time.time() - start))\n",
    "        \n",
    "#         # Remember best accuracy and save checkpoint\n",
    "#         is_best = val_acc.avg >= best_acc\n",
    "#         best_acc = max(val_acc.avg, best_acc)\n",
    "#         if is_best:\n",
    "#             early_stopping_counter = 0\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'state_dict': model.state_dict(),\n",
    "#                 'optimizer' : optimizer.state_dict(),\n",
    "#             }, 'model_best.pth.tar')\n",
    "#         else:\n",
    "#             early_stopping_counter += 1\n",
    "#             print(\"EarlyStopping: %i / %i\" % (early_stopping_counter, args.patience))\n",
    "#             if early_stopping_counter == args.patience:\n",
    "#                 print(\"EarlyStopping: Stop training\")\n",
    "#                 break\n",
    "\n",
    "#     print(\"Optimization finished!\")\n",
    "\n",
    "    # Testing\n",
    "    test_loss = AverageMeter()\n",
    "    test_acc = AverageMeter()\n",
    "    print(\"Loading checkpoint!\")\n",
    "    checkpoint = torch.load('model_best_3.pth.tar')\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    ## for averaging AUC\n",
    "    test_out_all = []\n",
    "    test_true_all = []\n",
    "    \n",
    "    for i in range(n_test_batches):\n",
    "        output, loss = test(adj_test[i], features_test[i], batch_n_graphs_test[i], y_test[i])\n",
    "        input('stop')\n",
    "        test_loss.update(loss.item(), output.size(0))\n",
    "        test_acc.update(accuracy(output.data, y_test[i].data), output.size(0))\n",
    "        # *****\n",
    "        test_out_all.extend(output.data.cpu()[:,1].tolist())\n",
    "        test_true_all.extend(y_test[i].data.cpu().tolist())\n",
    "        \n",
    "    accs.append(test_acc.avg.cpu().numpy())\n",
    "\n",
    "    # Print results\n",
    "    print(\"test_loss=\", \"{:.5f}\".format(test_loss.avg), \"test_acc=\", \"{:.5f}\".format(test_acc.avg))\n",
    "    print_metrics(test_true_all, test_out_all)\n",
    "    \n",
    "    viz_fpr, viz_tpr, thresholds = metrics.roc_curve(test_true_all, test_out_all)\n",
    "    interp_tpr = np.interp(mean_fpr, viz_fpr, viz_tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    false_positive_rate, recall, thresholds = roc_curve(test_true_all, test_out_all)\n",
    "    viz_roc_auc = auc(false_positive_rate, recall)\n",
    "    aucs.append(viz_roc_auc)\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    if run_only_once == 1: \n",
    "        break\n",
    "print(\"avg_test_acc=\", \"{:.5f}\".format(np.mean(accs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../../MS_shayan/MRN_Time_EDSS.pickle', 'rb') as f:\n",
    "    result_EDSS = pickle.load(f)\n",
    "    \n",
    "MPAD_embedding = np.load('MPAD_embedding.npy')\n",
    "\n",
    "result_MPAD_embedding = []\n",
    "for MRN, mpad_embed in zip(list(result_EDSS.keys()), MPAD_embedding):\n",
    "    result_MPAD_embedding.append((MRN, mpad_embed))\n",
    "    \n",
    "with open('result_MPAD_embedding.pickle', 'wb') as f:\n",
    "    pickle.dump(result_MPAD_embedding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
